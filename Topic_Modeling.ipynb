{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "topic_modeling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "beaqF1Ca-KfU"
      },
      "source": [
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from time import sleep\n",
        "import datetime\n",
        "import bs4\n",
        "import json \n",
        "\n",
        "\n",
        "def clean_string(column):\n",
        "    return column.apply(lambda x: x.replace(\"\\n\",'',2)).apply(lambda x: x.replace('  ',''))\n",
        "\n",
        "def scrape_reviews(PATH, n_pages, sleep_time = 0.3):\n",
        "\n",
        "\n",
        "    names = []\n",
        "    ratings = []\n",
        "    headers = []\n",
        "    reviews = []\n",
        "    dates = []\n",
        "    locations = []\n",
        "\n",
        "    for p in range(18):\n",
        "\n",
        "        sleep(sleep_time)\n",
        "\n",
        "        #http = requests.get(f'{PATH}{p}&stars=1&stars=5')\n",
        "        x= PATH+str(p+1)\n",
        "       \n",
        "        r = requests.get(x)\n",
        "         \n",
        "        bsoup = BeautifulSoup(r.text, 'html.parser')\n",
        "        base_url = 'https://fr.trustpilot.com/review/mcdonalds.fr'\n",
        "\n",
        "        review_containers = bsoup.find_all('div', class_ = 'review-content__body')\n",
        "        user_containers = bsoup.find_all('div', class_ = 'consumer-information__name')\n",
        "        rating_container = bsoup.find_all('div', class_=\"star-rating star-rating--medium\")      \n",
        "       \n",
        "        profile_link_containers = bsoup.find_all('aside', class_ = 'review__consumer-information' )\n",
        "        \n",
        "        for x in range(len(bsoup.find_all('div', class_ = 'review-content'))):\n",
        "            \n",
        "            review_c = review_containers[x]\n",
        "            headers.append(review_c.h2.a.text)\n",
        "            \n",
        "            if type(review_c.p) == bs4.element.Tag:\n",
        "                reviews.append(review_c.p.text)\n",
        "            else :\n",
        "                reviews.append(\"\")\n",
        "            reviewer = user_containers[x]\n",
        "          \n",
        "            rating = rating_container[x]\n",
        "            ratings.append( \"\\n\".join([img['alt'] for img in rating.find_all('img', alt=True)]).replace(\" étoile :\",\"\").replace(\" étoiles :\",\"\"))\n",
        "            \n",
        "           \n",
        "            \n",
        "            prof = profile_link_containers[x]\n",
        "            link = 'https://www.trustpilot.com'+ prof.a['href']\n",
        "            c_profile = requests.get(f'{link}')\n",
        "            csoup = BeautifulSoup(c_profile.text, 'html.parser')\n",
        "            cust_container = csoup.find('div', class_ = 'user-summary-location')\n",
        "            locations.append(cust_container.text)\n",
        "    \n",
        "        for star in  bsoup.find_all('div', class_ = 'review-content'):\n",
        "          \n",
        "        # Get date value\n",
        "          date_json = json.loads(star.find('script').text)\n",
        "          date = date_json['publishedDate']\n",
        "          dates.append(date)\n",
        "\n",
        "    rev_df = pd.DataFrame(list(zip( headers, reviews, ratings, ratings, dates, locations)),\n",
        "                  columns = ['Header','Review','Rating','Mention','Date', 'Location'])\n",
        "    \n",
        "    print(len(bsoup.find_all('div', class_ = 'review-content')))\n",
        "    \n",
        "    rev_df.Review = clean_string(rev_df.Review)\n",
        "    rev_df.Location = clean_string(rev_df.Location)\n",
        "    rev_df.Location = rev_df.Location.apply(lambda x: x.split(',',1)[-1])\n",
        "    rev_df.Rating = rev_df.Rating.str[:1]  \n",
        "    rev_df.Mention = rev_df.Mention.str[2:] \n",
        "    rev_df.Date = rev_df.Date.str[:10] \n",
        "    return rev_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YynChwMc-R6P",
        "outputId": "b9e3ad1f-5306-4e08-f848-d075a0208fef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df = scrape_reviews(PATH = 'https://fr.trustpilot.com/review/mcdonalds.fr?page=',\n",
        "                   n_pages = 18)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDDmlAOw-R91",
        "outputId": "d9495b2d-d7c8-4247-ece0-cf419ffc6636",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        }
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Header</th>\n",
              "      <th>Review</th>\n",
              "      <th>Rating</th>\n",
              "      <th>Mention</th>\n",
              "      <th>Date</th>\n",
              "      <th>Location</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Est ce que le covid empeche les…</td>\n",
              "      <td>Est ce que le covid empeche les employes de di...</td>\n",
              "      <td>1</td>\n",
              "      <td>mauvais</td>\n",
              "      <td>2020-05-18</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Macdo Cesson personnel incompétent</td>\n",
              "      <td>Bonjour je reviens du Macdonald de boisenart à...</td>\n",
              "      <td>1</td>\n",
              "      <td>mauvais</td>\n",
              "      <td>2020-05-17</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>De plus en plus petit les portions depuis le c...</td>\n",
              "      <td>De plus en plus petit depuis le confinement..m...</td>\n",
              "      <td>1</td>\n",
              "      <td>mauvais</td>\n",
              "      <td>2020-05-16</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Burger sans saveur sans salade sans…</td>\n",
              "      <td>Burger sans saveur sans salade sans tomates en...</td>\n",
              "      <td>1</td>\n",
              "      <td>mauvais</td>\n",
              "      <td>2020-05-15</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Macdonald’s plaisance du touch 31830</td>\n",
              "      <td>Macdonald’s plaisance du touch 31830 : NUL !!!...</td>\n",
              "      <td>1</td>\n",
              "      <td>mauvais</td>\n",
              "      <td>2020-05-15</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>346</th>\n",
              "      <td>Mon fast-food préféré</td>\n",
              "      <td>J'aime beaucoup aller chez McDonald, surtout q...</td>\n",
              "      <td>4</td>\n",
              "      <td>bien</td>\n",
              "      <td>2016-03-28</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>347</th>\n",
              "      <td>Degueulasse</td>\n",
              "      <td>Dégueulasse ! Frites immonde, service lent.Le ...</td>\n",
              "      <td>1</td>\n",
              "      <td>mauvais</td>\n",
              "      <td>2015-09-08</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>348</th>\n",
              "      <td>genial a part les sodas</td>\n",
              "      <td>c est sympa surtout pour que les enfants puiss...</td>\n",
              "      <td>5</td>\n",
              "      <td>excellent</td>\n",
              "      <td>2015-07-03</td>\n",
              "      <td>Malta</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349</th>\n",
              "      <td>pas toujours de glace</td>\n",
              "      <td>Effectivement, rarement de la glace le soir..</td>\n",
              "      <td>4</td>\n",
              "      <td>bien</td>\n",
              "      <td>2014-07-25</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>350</th>\n",
              "      <td>mac do Venette NUL</td>\n",
              "      <td>Plusieurs fois dans l'année, on s'arrête, et a...</td>\n",
              "      <td>1</td>\n",
              "      <td>mauvais</td>\n",
              "      <td>2012-04-22</td>\n",
              "      <td>France</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>351 rows × 6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Header  ... Location\n",
              "0                     Est ce que le covid empeche les…  ...   France\n",
              "1                   Macdo Cesson personnel incompétent  ...   France\n",
              "2    De plus en plus petit les portions depuis le c...  ...   France\n",
              "3                 Burger sans saveur sans salade sans…  ...   France\n",
              "4                 Macdonald’s plaisance du touch 31830  ...   France\n",
              "..                                                 ...  ...      ...\n",
              "346                              Mon fast-food préféré  ...   France\n",
              "347                                        Degueulasse  ...   France\n",
              "348                            genial a part les sodas  ...    Malta\n",
              "349                              pas toujours de glace  ...   France\n",
              "350                                 mac do Venette NUL  ...   France\n",
              "\n",
              "[351 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyrK4CcrE2Vd",
        "outputId": "fc48598f-8f34-4c42-ce88-a8a6c7a6681a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "df.write.format(\"csv\").save(\"//Desktop\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-b554089f5952>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"//Desktop\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'write'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z3jctT66elE",
        "outputId": "7e90d499-56d0-4bbf-8a0d-02da43f36e37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-26-3aaec68b3987>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    np.savetxt('xgboost.txt', df.values, fmt='%d', delimiter=\"\\t\"\")\u001b[0m\n\u001b[0m                                                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JImUDt0-SBP",
        "outputId": "c0f3a57e-d2d5-4f91-b853-7407fdee8dbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys\n",
        "# !{sys.executable} -m spacy download en\n",
        "import re, numpy as np, pandas as pd\n",
        "from pprint import pprint\n",
        "\n",
        "# Gensim\n",
        "import gensim\n",
        "import spacy\n",
        "import logging\n",
        "import warnings\n",
        "import gensim.corpora as corpora\n",
        "from gensim.utils import lemmatize, simple_preprocess\n",
        "from gensim.models import CoherenceModel\n",
        "import matplotlib.pyplot as plt\n",
        "import gensim.models \n",
        "\n",
        "# NLTK Stop words\n",
        "from nltk.corpus import stopwords\n",
        "#from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop \n",
        "stop_words = stopwords.words('french')\n",
        "stop_words.extend(['ca', 'ete','donc','apres','tout','cest','quand','très','ça','daller','plus','do','fois','plus','car','tous','alors','tres','lors','faire','sans','dautre','nest','encore','comment','choose','prendre','arrive','meme','avant','etaient','ver','meet','toute','vraiment','toujour','tient','dire','quelque','fait','renverse'])\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
        "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-bc261c4a3b45>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m#from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mstop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'french'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ca'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ete'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'donc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'apres'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tout'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'cest'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'quand'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'très'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ça'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'daller'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'plus'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'do'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fois'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'plus'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'car'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tous'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'alors'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tres'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'lors'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'faire'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'sans'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dautre'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'nest'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'encore'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'comment'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'choose'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'prendre'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'arrive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'meme'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'avant'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'etaient'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ver'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'meet'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'toute'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'vraiment'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'toujour'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tient'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'dire'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'quelque'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'fait'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'renverse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LEeE6pw-SES"
      },
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EAvm1yYi-SHj"
      },
      "source": [
        "def sent_to_words(sentences):\n",
        "    for sent in sentences:\n",
        "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
        "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
        "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
        "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
        "        yield(sent)  \n",
        "\n",
        "# Convert to list\n",
        "data = df.Review.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "print(data_words[:1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPj82_4f8Tsc"
      },
      "source": [
        "\n",
        "!python3 -m spacy download fr_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhY9B3EX8gFw"
      },
      "source": [
        "import fr_core_news_sm\n",
        "nlp = fr_core_news_sm.load()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gyu2dTP4-SKz"
      },
      "source": [
        "# Build the bigram and trigram models\n",
        "from nltk.stem.snowball import FrenchStemmer\n",
        "Stemmer = FrenchStemmer()\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
        "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
        "\n",
        "# !python3 -m spacy download en  # run in terminal once\n",
        "# or do\n",
        "# !conda install -c conda-forge spacy-model-en_core_web_md \n",
        "# and use nlp=spacy.load('en_core_web_sm') instead in below function.\n",
        "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB']):\n",
        "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and Lemmatization\"\"\"\n",
        "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
        "    texts = [bigram_mod[doc] for doc in texts]\n",
        "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
        "    texts_out = []\n",
        "    nlp = fr_core_news_sm.load()\n",
        "    for sent in texts:\n",
        "        doc = nlp(\" \".join(sent)) \n",
        "        texts_out.append([Stemmer.stem(token.text) for token in doc if token.pos_ in allowed_postags])\n",
        "    # remove stopwords once more after lemmatization\n",
        "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
        "    return texts_out\n",
        "\n",
        "\n",
        "data_ready = process_words(data_words)  # processed Text Data!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA9BjNXt9_2h"
      },
      "source": [
        "print(data_ready)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKfrJCG6VLTo"
      },
      "source": [
        "# Load the library with the CountVectorizer method\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('whitegrid')\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_10_most_common_words(count_data, count_vectorizer):\n",
        "    import matplotlib.pyplot as plt\n",
        "    words = count_vectorizer.get_feature_names()\n",
        "    total_counts = np.zeros(len(words))\n",
        "    for t in count_data:\n",
        "        total_counts+=t.toarray()[0]\n",
        "    \n",
        "    count_dict = (zip(words, total_counts))\n",
        "    count_dict = sorted(count_dict, key=lambda x:x[1], reverse=True)[0:10]\n",
        "    words = [w[0] for w in count_dict]\n",
        "    counts = [w[1] for w in count_dict]\n",
        "    x_pos = np.arange(len(words)) \n",
        "    \n",
        "    plt.figure(2, figsize=(15, 15/1.6180))\n",
        "    plt.subplot(title='10 most common words')\n",
        "    sns.set_context(\"notebook\", font_scale=1.25, rc={\"lines.linewidth\": 2.5})\n",
        "    sns.barplot(x_pos, counts, palette='husl')\n",
        "    plt.xticks(x_pos, words, rotation=90) \n",
        "    plt.xlabel('words')\n",
        "    plt.ylabel('counts')\n",
        "    plt.show()\n",
        "# Initialise the count vectorizer with the English stop words\n",
        "count_vectorizer = CountVectorizer(stop_words=stop_words )\n",
        "# Fit and transform the processed titles\n",
        "count_data = count_vectorizer.fit_transform(data)\n",
        "# Visualise the 10 most common words\n",
        "plot_10_most_common_words(count_data, count_vectorizer)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7vIrobFVMtW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ok07v6I-SOK"
      },
      "source": [
        "# Create Dictionary\n",
        "from gensim import corpora, models\n",
        "id2word = corpora.Dictionary(data_ready)\n",
        "id2word.filter_extremes(no_below=7, no_above=0.8, keep_n=1000000)\n",
        "# Create Corpus: Term Document Frequency\n",
        "corpus = [id2word.doc2bow(text) for text in data_ready]\n",
        "tfidf = models.TfidfModel(corpus)\n",
        "corpus_tfidf = tfidf[corpus]\n",
        "\n",
        "# Build LDA model\n",
        "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus_tfidf,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=4, \n",
        "                                           random_state=70,\n",
        "                                           update_every=1,\n",
        "                                           chunksize=20,\n",
        "                                           passes=100,\n",
        "                                           alpha='symmetric',\n",
        "                                           iterations=100,\n",
        "                                           per_word_topics=True)\n",
        "\n",
        "pprint(lda_model.print_topics())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aw-kYm3dMYjR"
      },
      "source": [
        "num_topic=4\n",
        "def get_lda_topics(model, num_topics):\n",
        "    word_dict = {};\n",
        "    for i in range(num_topics):\n",
        "        words = model.show_topic(i, topn = 20);\n",
        "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = [i[0] for i in words];\n",
        "    return pd.DataFrame(word_dict);\n",
        "get_lda_topics(lda_model, num_topic)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYaYigKDNF05"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyvWa26fQgpp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5TpyJOH3Qg84"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mi_P6137BrHD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xVEtiTWt-SRY"
      },
      "source": [
        "def format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n",
        "    # Init output\n",
        "    sent_topics_df = pd.DataFrame()\n",
        "\n",
        "    # Get main topic i each document\n",
        "    for i, row_list in enumerate(ldamodel[corpus]):\n",
        "        row = row_list[0] if ldamodel.per_word_topics else row_list            \n",
        "        # print(row)\n",
        "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
        "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
        "        for j, (topic_num, prop_topic) in enumerate(row):\n",
        "            if j == 0:  # => dominant topic\n",
        "                wp = ldamodel.show_topic(topic_num)\n",
        "                topic_keywords = \", \".join([word for word, prop in wp])\n",
        "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
        "            else:\n",
        "                break\n",
        "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
        "\n",
        "    # Add original text to the end of the output\n",
        "    contents = pd.Series(texts)\n",
        "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
        "    return(sent_topics_df)\n",
        "\n",
        "\n",
        "df_topic_sents_keywords = format_topics_sentences(ldamodel=lda_model, corpus=corpus, texts=data_ready)\n",
        "# Format\n",
        "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
        "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
        "df_dominant_topic.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWRxq5K8tJm9"
      },
      "source": [
        "Aspect =pd.DataFrame(pd.cut(df_dominant_topic['Dominant_Topic'],bins=[-1,0,1,2,3],labels=['Restaurant en general ','Service','Commande/drive','Nourriture']))\n",
        "\n",
        "Aspect.columns=[\"Aspect\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_y_rkuStJxU"
      },
      "source": [
        "result = pd.concat([df, Aspect], axis=1)\n",
        "result.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gxVGyDOtJ3J"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh0yh3ca-SUt"
      },
      "source": [
        "# Display setting to show more characters in column\n",
        "pd.options.display.max_colwidth = 100\n",
        "\n",
        "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
        "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
        "\n",
        "for i, grp in sent_topics_outdf_grpd:\n",
        "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
        "                                             grp.sort_values(['Perc_Contribution'], ascending=False).head(1)], \n",
        "                                            axis=0)\n",
        "\n",
        "# Reset Index    \n",
        "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
        "\n",
        "# Format\n",
        "sent_topics_sorteddf_mallet.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Representative Text\"]\n",
        "\n",
        "# Show\n",
        "sent_topics_sorteddf_mallet.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v12j68bX-SX0"
      },
      "source": [
        "# 1. Wordcloud of Top N words in each topic\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import matplotlib.colors as mcolors\n",
        "\n",
        "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
        "\n",
        "cloud = WordCloud(stopwords=stop_words,\n",
        "                  background_color='white',\n",
        "                  width=2500,\n",
        "                  height=1800,\n",
        "                  max_words=10,\n",
        "                  colormap='tab10',\n",
        "                  color_func=lambda *args, **kwargs: cols[i],\n",
        "                  prefer_horizontal=1.0)\n",
        "\n",
        "topics = lda_model.show_topics(formatted=False)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
        "\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    fig.add_subplot(ax)\n",
        "    topic_words = dict(topics[i][1])\n",
        "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
        "    plt.gca().imshow(cloud)\n",
        "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
        "    plt.gca().axis('off')\n",
        "\n",
        "\n",
        "plt.subplots_adjust(wspace=0, hspace=0)\n",
        "plt.axis('off')\n",
        "plt.margins(x=0, y=0)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izx3m4yO-Sa5"
      },
      "source": [
        "from collections import Counter\n",
        "topics = lda_model.show_topics(formatted=False)\n",
        "data_flat = [w for w_list in data_ready for w in w_list]\n",
        "counter = Counter(data_flat)\n",
        "\n",
        "out = []\n",
        "for i, topic in topics:\n",
        "    for word, weight in topic:\n",
        "        out.append([word, i , weight, counter[word]])\n",
        "\n",
        "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
        "\n",
        "# Plot Word Count and Weights of Topic Keywords\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
        "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    #ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
        "    ax_twin = ax.twinx()\n",
        "    ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
        "    #ax.set_ylabel('Word Count', color=cols[i])\n",
        "    ax_twin.set_ylim(0, 0.13); \n",
        "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
        "    ax.tick_params(axis='y', left=False)\n",
        "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
        "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
        "\n",
        "fig.tight_layout(w_pad=2)    \n",
        "fig.suptitle(' Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRD7H58Bgukj"
      },
      "source": [
        "from collections import Counter\n",
        "topics = lda_model.show_topics(formatted=False)\n",
        "data_flat = [w for w_list in data_ready for w in w_list]\n",
        "counter = Counter(data_flat)\n",
        "\n",
        "out = []\n",
        "for i, topic in topics:\n",
        "    for word, weight in topic:\n",
        "        out.append([word, i , weight, counter[word]])\n",
        "\n",
        "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
        "\n",
        "# Plot Word Count and Weights of Topic Keywords\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
        "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
        "for i, ax in enumerate(axes.flatten()):\n",
        "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
        "    ax_twin = ax.twinx()\n",
        "    #ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
        "    ax.set_ylabel('Word Count', color=cols[i])\n",
        "    #ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
        "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
        "    ax.tick_params(axis='y', left=False)\n",
        "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
        "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
        "\n",
        "fig.tight_layout(w_pad=2)    \n",
        "fig.suptitle('Word Count for each topic', fontsize=22, y=1.05)    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXBDld60-SeH"
      },
      "source": [
        "# Sentence Coloring of N Sentences\n",
        "from matplotlib.patches import Rectangle\n",
        "\n",
        "def sentences_chart(lda_model=lda_model, corpus=corpus_tfidf, start = 70, end = 77):\n",
        "    corp = corpus[start:end]\n",
        "    mycolors = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
        "\n",
        "    fig, axes = plt.subplots(end-start, 1, figsize=(20, (end-start)*0.95), dpi=160)       \n",
        "    axes[0].axis('off')\n",
        "    for i, ax in enumerate(axes):\n",
        "        if i > 0:\n",
        "            corp_cur = corp[i-1] \n",
        "            topic_percs, wordid_topics, wordid_phivalues = lda_model[corp_cur]\n",
        "            word_dominanttopic = [(lda_model.id2word[wd], topic[0]) for wd, topic in wordid_topics]    \n",
        "            ax.text(0.01, 0.5, \"Doc \" + str(i-1) + \": \", verticalalignment='center',\n",
        "                    fontsize=16, color='black', transform=ax.transAxes, fontweight=700)\n",
        "\n",
        "            # Draw Rectange\n",
        "            topic_percs_sorted = sorted(topic_percs, key=lambda x: (x[1]), reverse=True)\n",
        "            ax.add_patch(Rectangle((0.0, 0.05), 0.99, 0.90, fill=None, alpha=1, \n",
        "                                   color=mycolors[topic_percs_sorted[0][0]], linewidth=2))\n",
        "\n",
        "            word_pos = 0.06\n",
        "            for j, (word, topics) in enumerate(word_dominanttopic):\n",
        "                if j < 14:\n",
        "                    ax.text(word_pos, 0.5, word,\n",
        "                            horizontalalignment='left',\n",
        "                            verticalalignment='center',\n",
        "                            fontsize=16, color=mycolors[topics],\n",
        "                            transform=ax.transAxes, fontweight=700)\n",
        "                    word_pos += .009 * len(word)  # to move the word for the next iter\n",
        "                    ax.axis('off')\n",
        "            ax.text(word_pos, 0.5, '. . .',\n",
        "                    horizontalalignment='left',\n",
        "                    verticalalignment='center',\n",
        "                    fontsize=16, color='black',\n",
        "                    transform=ax.transAxes)       \n",
        "\n",
        "    plt.subplots_adjust(wspace=0, hspace=0)\n",
        "    plt.suptitle('Sentence Topic Coloring for Documents: ' + str(start) + ' to ' + str(end-2), fontsize=22, y=0.95, fontweight=700)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "sentences_chart()    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQ-pvoxT-ShU"
      },
      "source": [
        "# Sentence Coloring of N Sentences\n",
        "def topics_per_document(model, corpus, start=0, end=1):\n",
        "    corpus_sel = corpus[start:end]\n",
        "    dominant_topics = []\n",
        "    topic_percentages = []\n",
        "    for i, corp in enumerate(corpus_sel):\n",
        "        topic_percs, wordid_topics, wordid_phivalues = model[corp]\n",
        "        dominant_topic = sorted(topic_percs, key = lambda x: x[1], reverse=True)[0][0]\n",
        "        dominant_topics.append((i, dominant_topic))\n",
        "        topic_percentages.append(topic_percs)\n",
        "    return(dominant_topics, topic_percentages)\n",
        "\n",
        "dominant_topics, topic_percentages = topics_per_document(model=lda_model, corpus=corpus, end=-1)            \n",
        "\n",
        "# Distribution of Dominant Topics in Each Document\n",
        "df = pd.DataFrame(dominant_topics, columns=['Document_Id', 'Dominant_Topic'])\n",
        "dominant_topic_in_each_doc = df.groupby('Dominant_Topic').size()\n",
        "df_dominant_topic_in_each_doc = dominant_topic_in_each_doc.to_frame(name='count').reset_index()\n",
        "\n",
        "# Total Topic Distribution by actual weight\n",
        "topic_weightage_by_doc = pd.DataFrame([dict(t) for t in topic_percentages])\n",
        "df_topic_weightage_by_doc = topic_weightage_by_doc.sum().to_frame(name='count').reset_index()\n",
        "\n",
        "# Top 3 Keywords for each Topic\n",
        "topic_top3words = [(i, topic) for i, topics in lda_model.show_topics(formatted=False) \n",
        "                                 for j, (topic, wt) in enumerate(topics) if j < 3]\n",
        "\n",
        "df_top3words_stacked = pd.DataFrame(topic_top3words, columns=['topic_id', 'words'])\n",
        "df_top3words = df_top3words_stacked.groupby('topic_id').agg(', \\n'.join)\n",
        "df_top3words.reset_index(level=0,inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PYb_fu4-Skh"
      },
      "source": [
        "from matplotlib.ticker import FuncFormatter\n",
        "\n",
        "# Plot\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4), dpi=120, sharey=True)\n",
        "\n",
        "# Topic Distribution by Dominant Topics\n",
        "ax1.bar(x='Dominant_Topic', height='count', data=df_dominant_topic_in_each_doc, width=.5, color='firebrick')\n",
        "ax1.set_xticks(range(df_dominant_topic_in_each_doc.Dominant_Topic.unique().__len__()))\n",
        "tick_formatter = FuncFormatter(lambda x, pos: 'Topic ' + str(x)+ '\\n' + df_top3words.loc[df_top3words.topic_id==x, 'words'].values[0])\n",
        "ax1.xaxis.set_major_formatter(tick_formatter)\n",
        "ax1.set_title('Number of Documents by Dominant Topic', fontdict=dict(size=10))\n",
        "ax1.set_ylabel('Number of Documents')\n",
        "ax1.set_ylim(0, 200)\n",
        "\n",
        "# Topic Distribution by Topic Weights\n",
        "ax2.bar(x='index', height='count', data=df_topic_weightage_by_doc, width=.5, color='steelblue')\n",
        "ax2.set_xticks(range(df_topic_weightage_by_doc.index.unique().__len__()))\n",
        "ax2.xaxis.set_major_formatter(tick_formatter)\n",
        "ax2.set_title('Number of Documents by Topic Weightage', fontdict=dict(size=10))\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXH-0lgvY7gN"
      },
      "source": [
        "# Number of Documents for Each Topic\n",
        "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
        "\n",
        "# Percentage of Documents for Each Topic\n",
        "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
        "\n",
        "# Topic Number and Keywords\n",
        "topic_num_keywords = df_topic_sents_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
        "\n",
        "# Concatenate Column wise\n",
        "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
        "\n",
        "# Change Column names\n",
        "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
        "\n",
        "# Show\n",
        "df_dominant_topics[:3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrrZYN9D-SoB"
      },
      "source": [
        "import pyLDAvis.gensim\n",
        "pyLDAvis.enable_notebook()\n",
        "vis = pyLDAvis.gensim.prepare(lda_model, corpus, dictionary=lda_model.id2word)\n",
        "vis\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om-8WBXV-SrV"
      },
      "source": [
        "!pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkVWc2yxi8oU"
      },
      "source": [
        "##  Topic modeling with NMF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PRcuuIy7-Suz"
      },
      "source": [
        "import pandas as pd;\n",
        "import numpy as np;\n",
        "import scipy as sp;\n",
        "import sklearn;\n",
        "import sys;\n",
        "from nltk.corpus import stopwords;\n",
        "import nltk;\n",
        "from gensim.models import ldamodel\n",
        "import gensim.corpora;\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer;\n",
        "from sklearn.decomposition import NMF;\n",
        "from sklearn.preprocessing import normalize;\n",
        "import pickle;"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cht2Vebj-Syd"
      },
      "source": [
        "train_headlines_sentences = [' '.join(text) for text in data_ready]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88oeO0bXkCKG"
      },
      "source": [
        "vectorizer = CountVectorizer(analyzer='word', max_features=5000);\n",
        "x_counts = vectorizer.fit_transform(train_headlines_sentences);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_xFh5v2-S17"
      },
      "source": [
        "transformer = TfidfTransformer(smooth_idf=False);\n",
        "x_tfidf = transformer.fit_transform(x_counts);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJekpEML-S5f"
      },
      "source": [
        "xtfidf_norm = normalize(x_tfidf, norm='l1', axis=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6qnd3cZ-S9W"
      },
      "source": [
        "#obtain a NMF model.\n",
        "model = NMF(n_components=4, init='nndsvd',shuffle=True);\n",
        "#fit the model\n",
        "model.fit(xtfidf_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eac8az41-TA1"
      },
      "source": [
        "def get_nmf_topics(model, n_top_words):\n",
        "    \n",
        "    #the word ids obtained need to be reverse-mapped to the words so we can print the topic names.\n",
        "    feat_names = vectorizer.get_feature_names()\n",
        "    \n",
        "    word_dict = {};\n",
        "    for i in range(4):\n",
        "        \n",
        "        #for each topic, obtain the largest values, and add the words they map to into the dictionary.\n",
        "        words_ids = model.components_[i].argsort()[:-10 - 1:-1]\n",
        "        words = [feat_names[key] for key in words_ids]\n",
        "        word_dict['Topic # ' + '{:02d}'.format(i+1)] = words;\n",
        "    \n",
        "    return pd.DataFrame(word_dict);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Woi7plTa-TEL"
      },
      "source": [
        "get_nmf_topics(model,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrI9xw2V-TH1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umFEqHDlDN-d"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "50T7hbwA-TLN"
      },
      "source": [
        "####################################### NEXT STEP OF OPINION MINING ###########################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-opqo5U-TOp"
      },
      "source": [
        "result.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KAreuMx-TSF"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plot_size = plt.rcParams[\"figure.figsize\"] \n",
        "print(plot_size[0]) \n",
        "print(plot_size[1])\n",
        "\n",
        "plot_size[0] = 10\n",
        "plot_size[1] = 10\n",
        "plt.rcParams[\"figure.figsize\"] = plot_size \n",
        "result.Mention.value_counts().plot(kind='pie', autopct='%1.0f%%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3PU9Kne-TVT"
      },
      "source": [
        "\n",
        "list2=[]\n",
        "\n",
        "for line in result['Rating']:   \n",
        "    if line in [\"1\",\"2\"]:\n",
        "      list2.append(\"negative\")\n",
        "    elif line ==\"3\":\n",
        "      list2.append(\"neutre\")\n",
        "    else :\n",
        "       list2.append(\"positive\")\n",
        "\n",
        "            \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pJ14IfZG-TYf"
      },
      "source": [
        "!pip install vaderSentiment"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBEvTWke-Tbz"
      },
      "source": [
        "sentiment= pd.DataFrame(list2)\n",
        "sentiment.columns=[\"sentiment\"]\n",
        "resultFinal = pd.concat([result, sentiment], axis=1)\n",
        "resultFinal.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQrsjECo-TfA"
      },
      "source": [
        "print(sentiment.groupby('sentiment').size().sort_values(ascending=False))\n",
        "resultFinal.sentiment.value_counts().plot(kind='pie', autopct='%1.0f%%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kr2aAtT4nYHX"
      },
      "source": [
        "resultFinal.Aspect.value_counts().plot(kind='pie', autopct='%1.0f%%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xez0uk3-Tin"
      },
      "source": [
        "# reviews_train.columns\n",
        "print(resultFinal.groupby('Aspect').size().sort_values(ascending=False))\n",
        "\n",
        "#how many categories\n",
        "print(\"number of categories\",resultFinal.Aspect.nunique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZ3_c3cV-Tln"
      },
      "source": [
        "resultFinalWork=resultFinal\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aLbEsFKb1A0"
      },
      "source": [
        "!python3 -m spacy download fr_core_news_md"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pO0oFgc8FNfr"
      },
      "source": [
        "### ***TRY AMA BAED BICH NAHEHA KEN MAMCHETICH ***\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8ecr5AG-To0"
      },
      "source": [
        "import spacy\n",
        "import fr_core_news_md\n",
        "nlp = fr_core_news_md.load()\n",
        "\n",
        "resultFinalWork.Review = resultFinalWork.Review.str.lower()\n",
        "\n",
        "aspect_terms = []\n",
        "for review in nlp.pipe(resultFinalWork.Review):\n",
        "    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n",
        "    aspect_terms.append(' '.join(chunks))\n",
        "resultFinalWork['aspect_terms'] = aspect_terms\n",
        "resultFinalWork.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otgUakrw-Trw"
      },
      "source": [
        "from keras.models import load_model\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "\n",
        "aspect_categories_model = Sequential()\n",
        "aspect_categories_model.add(Dense(512, input_shape=(6000,), activation='relu'))\n",
        "aspect_categories_model.add(Dense(4, activation='softmax'))\n",
        "aspect_categories_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCxSzbv8rurh"
      },
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "vocab_size = 6000 # We set a maximum size for the vocabulary\n",
        "tokenizer = Tokenizer(num_words=vocab_size)\n",
        "tokenizer.fit_on_texts(resultFinalWork.Review)\n",
        "aspect_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(resultFinalWork.aspect_terms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awuizdNKGg6R"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "integer_category = label_encoder.fit_transform(resultFinalWork.Aspect)\n",
        "dummy_category = to_categorical(integer_category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-eTNnmeGg8-"
      },
      "source": [
        "aspect_categories_model.fit(aspect_tokenized, dummy_category, epochs=6, verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egrgOGf7GhCw"
      },
      "source": [
        "new_review = \"le service est horrible \"\n",
        "\n",
        "chunks = [(chunk.root.text) for chunk in nlp(new_review).noun_chunks if chunk.root.pos_ == 'NOUN']\n",
        "new_review_aspect_terms = ' '.join(chunks)\n",
        "new_review_aspect_tokenized = tokenizer.texts_to_matrix([new_review_aspect_terms])\n",
        "\n",
        "new_review_category = label_encoder.inverse_transform(aspect_categories_model.predict_classes(new_review_aspect_tokenized))\n",
        "print(new_review_category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBSO_sR2GhF7"
      },
      "source": [
        "sentiment_terms = []\n",
        "for review in nlp.pipe(resultFinalWork['Review']):\n",
        "        if review.is_parsed:\n",
        "            sentiment_terms.append(' '.join([token.text for token in review if (not token.is_stop and not token.is_punct and (token.pos_ == \"ADJ\" or token.pos_ == \"VERB\"))]))\n",
        "        else:\n",
        "            sentiment_terms.append('')  \n",
        "resultFinalWork['sentiment_terms'] = sentiment_terms\n",
        "resultFinalWork.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wH-SrPL_24qH",
        "outputId": "84e35cbf-2989-484d-e551-224e94353f81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        }
      },
      "source": [
        "resultFinalWork.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-9fdd6631b25e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresultFinalWork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'resultFinalWork' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTNu4N7q24-k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDVfrh6e25F-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esFGIFuQoCoi"
      },
      "source": [
        "Trutpilot_sentiment = resultFinalWork.groupby(['Aspect', 'sentiment']).sentiment.count().unstack()\n",
        "Trutpilot_sentiment.plot(kind='bar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RAcagUhEGhJY"
      },
      "source": [
        "sentiment_model = Sequential()\n",
        "sentiment_model.add(Dense(512, input_shape=(6000,), activation='relu'))\n",
        "sentiment_model.add(Dense(218, input_shape=(6000,), activation='relu'))\n",
        "sentiment_model.add(Dense(64, input_shape=(6000,), activation='relu'))\n",
        "sentiment_model.add(Dense(3, activation='softmax'))\n",
        "sentiment_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xKwDxA1GhL7"
      },
      "source": [
        "sentiment_tokenized = pd.DataFrame(tokenizer.texts_to_matrix(resultFinalWork.sentiment_terms))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tXQtNiHGhPa"
      },
      "source": [
        "label_encoder_2 = LabelEncoder()\n",
        "integer_sentiment = label_encoder_2.fit_transform(resultFinalWork.sentiment)\n",
        "dummy_sentiment = to_categorical(integer_sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh-CT1U-GhSK"
      },
      "source": [
        "sentiment_model.fit(sentiment_tokenized, dummy_sentiment, epochs=5, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zACAqzfPGhVP"
      },
      "source": [
        "new_review = \"mauvais service\"\n",
        "\n",
        "chunks = [(chunk.root.text) for chunk in nlp(new_review).noun_chunks if chunk.root.pos_ == 'NOUN']\n",
        "new_review_aspect_terms = ' '.join(chunks)\n",
        "new_review_aspect_tokenized = tokenizer.texts_to_matrix([new_review_aspect_terms])\n",
        "\n",
        "new_review_category = label_encoder_2.inverse_transform(sentiment_model.predict_classes(new_review_aspect_tokenized))\n",
        "print(new_review_category)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXc53NosZFo8"
      },
      "source": [
        "test_reviews = [\n",
        "    \"Bon service rapide.\",\n",
        "    \"L'hôtesse était très desagréable.\",\n",
        "    \"Le pain était rassis, la salade était trop chère et vide.\",\n",
        "    \"La nourriture que nous avons commandée était mauvais, même si je ne dirais pas que les margaritas étaient quelque chose à raconter.\",\n",
        "    \"Cet endroit a un décor totalement bizarre, des escaliers avec des murs en miroir - je suis surpris de voir que personne ne s'est encore cassé la tête ou n'est tombé des escaliers\"\n",
        "]\n",
        "\n",
        "# Aspect preprocessing\n",
        "test_reviews = [review.lower() for review in test_reviews]\n",
        "test_aspect_terms = []\n",
        "for review in nlp.pipe(test_reviews):\n",
        "    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n",
        "    test_aspect_terms.append(' '.join(chunks))\n",
        "test_aspect_terms = pd.DataFrame(tokenizer.texts_to_matrix(test_aspect_terms))\n",
        "                             \n",
        "# Sentiment preprocessing\n",
        "test_sentiment_terms = []\n",
        "for review in nlp.pipe(test_reviews):\n",
        "        if review.is_parsed:\n",
        "            test_sentiment_terms.append(' '.join([token.lemma_ for token in review if (not token.is_stop and not token.is_punct and (token.pos_ == \"ADJ\" or token.pos_ == \"VERB\"))]))\n",
        "        else:\n",
        "            test_sentiment_terms.append('') \n",
        "test_sentiment_terms = pd.DataFrame(tokenizer.texts_to_matrix(test_sentiment_terms))\n",
        "\n",
        "# Models output\n",
        "test_aspect_categories = label_encoder.inverse_transform(aspect_categories_model.predict_classes(test_aspect_terms))\n",
        "test_sentiment = label_encoder_2.inverse_transform(sentiment_model.predict_classes(test_sentiment_terms))\n",
        "for i in range(5):\n",
        "    print(\"Review \" + str(i+1) + \" is expressing a  \" + test_sentiment[i] + \" opinion about \" + test_aspect_categories[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-hsDeHoZFtW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBumDj1ErThQ"
      },
      "source": [
        "# ***Analyse du concurant***\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqDMi5rVfxDO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9SJW-7bfxGh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nox1wMkcfxKe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_dd-xqOfxNX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnGMwtO5fxQZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydxqWWldfxT1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdcwDkVDfxXR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hXZF3XSfxdV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vzmxc9kgfxgZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vk3--cpqfxj9"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHVtIwkufxnd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CRtZZE7fxrG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz8Op9RFfxvL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BxLSpGrAfx5r"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWPjzzktfx9V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6F5lRSn0fxau"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koybdysKZFw1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jYjht_EZGA5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGViVhekGhY7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}